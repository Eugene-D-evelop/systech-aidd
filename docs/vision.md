# Техническое видение проекта

## 1. Технологии

### Основные технологии
- **Python 3.11+** - основной язык разработки
- **uv** - управление зависимостями и виртуальным окружением
- **aiogram 3.x** - асинхронный фреймворк для Telegram Bot API (polling режим)
- **openai** - клиент для работы с LLM через OpenRouter
- **make** - автоматизация задач сборки и запуска

### Дополнительные библиотеки
- **pydantic** - валидация конфигурации и данных
- **python-dotenv** - управление переменными окружения
- **aiohttp** - асинхронные HTTP-запросы (зависимость aiogram)

### Инструменты разработки
- **ruff** - линтер и форматтер кода

## 2. Принципы разработки

### Основные принципы
1. **KISS (Keep It Simple, Stupid)** - максимальная простота решений
2. **ООП** - объектно-ориентированный подход с четкой структурой
3. **1 класс = 1 файл** - каждый класс в отдельном файле
4. **Асинхронность** - async/await для работы с Telegram и LLM API
5. **Конфигурация через переменные окружения** - .env файл для настроек
6. **Минимум абстракций** - только необходимые уровни абстракции
7. **Явное лучше неявного** - понятный и читаемый код

### Что НЕ делаем (антипаттерны для MVP)
- ❌ Сложные паттерны проектирования (фабрики, стратегии и т.д.)
- ❌ Избыточная модульность
- ❌ Преждевременная оптимизация
- ❌ Множественные уровни абстракции
- ❌ Базы данных (пока не нужны)

### Подход к реализации
- Прямолинейный код
- Минимальная вложенность классов
- Понятные имена классов и методов
- Каждый компонент делает одну вещь хорошо

## 3. Структура проекта

```
systech-aidd/
├── .env.example          # Пример конфигурации
├── .env                  # Реальная конфигурация (в .gitignore)
├── .gitignore
├── README.md
├── Makefile              # Команды для запуска и управления
├── pyproject.toml        # Конфигурация проекта для uv
├── uv.lock               # Lockfile зависимостей
├── docs/
│   ├── idea.md
│   └── vision.md
├── src/
│   ├── main.py           # Точка входа приложения
│   ├── config.py         # Класс Config - конфигурация из .env
│   ├── bot.py            # Класс TelegramBot - инициализация бота
│   ├── handlers.py       # Класс MessageHandler - обработка сообщений
│   ├── llm_client.py     # Класс LLMClient - работа с OpenRouter
│   └── conversation.py   # Класс Conversation - контекст диалога
└── tests/
    ├── __init__.py
    ├── test_config.py
    ├── test_handlers.py
    ├── test_llm_client.py
    └── test_conversation.py
```

### Описание компонентов

**Основные модули:**
1. **main.py** - запуск приложения, инициализация и старт polling
2. **config.py** - загрузка и валидация конфигурации (токены, API ключи)
3. **bot.py** - инициализация Telegram бота и регистрация handlers
4. **handlers.py** - обработчики Telegram сообщений (/start, текстовые сообщения)
5. **llm_client.py** - взаимодействие с LLM через OpenRouter
6. **conversation.py** - управление историей диалога пользователя (в памяти)

**Тестирование:**
- Юнит-тесты для каждого основного компонента
- Использование pytest с async поддержкой
- Моки для внешних API (Telegram, OpenRouter)

**Всего 6 файлов кода + тесты** - минимально и достаточно для MVP.

## 4. Архитектура проекта

### Архитектурная схема

```
┌─────────────────────────────────────────┐
│           main.py (Entry Point)         │
└────────────────┬────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────┐
│         config.py (Config)              │ ◄── .env
└─────────────────────────────────────────┘

         ┌───────┴───────┐
         ▼               ▼
┌──────────────┐  ┌──────────────┐
│  bot.py      │  │ llm_client.py│
│(TelegramBot) │  │  (LLMClient) │
└──────┬───────┘  └──────┬───────┘
       │                 │
       ▼                 │
┌──────────────┐         │
│ handlers.py  │         │
│(MessageHandler)◄───────┘
└──────┬───────┘
       │
       ▼
┌──────────────┐
│conversation.py│
│(Conversation)│
└──────────────┘
```

### Поток данных

1. **main.py** → Загружает Config → Создает TelegramBot и LLMClient → Запускает polling
2. **TelegramBot** → Регистрирует MessageHandler
3. **User Message** → MessageHandler → Conversation (добавляет в историю) → LLMClient (запрос к LLM) → Response → User
4. **Conversation** → Хранит историю сообщений в памяти (словарь: "chat_id:user_id" → список сообщений)

### Принципы взаимодействия

- **Config** - независимый, используется всеми
- **TelegramBot** - зависит только от Config
- **LLMClient** - зависит только от Config
- **MessageHandler** - зависит от LLMClient и Conversation
- **Conversation** - независимый, простое хранилище в памяти

### Особенности

- Нет БД - всё в памяти (при перезапуске история теряется)
- Нет сложных зависимостей между компонентами
- Каждый класс имеет четкую ответственность
- Асинхронная обработка через async/await

## 5. Модель данных

### Config (Pydantic BaseSettings)

```python
telegram_bot_token: str           # Токен Telegram бота
openrouter_api_key: str           # API ключ OpenRouter
openrouter_model: str             # Модель LLM (например, "anthropic/claude-3.5-sonnet")
system_prompt: str                # Системный промпт (роль ассистента)
max_history_length: int = 10     # Максимум сообщений в истории
temperature: float = 0.7          # Температура для LLM (креативность)
max_tokens: int = 1000            # Максимум токенов в ответе
timeout: int = 60                 # Таймаут запросов к API (секунды)
```

### Message (простой dict)

```python
{
    "role": str,        # "user" | "assistant" | "system"
    "content": str,     # Текст сообщения
    "timestamp": float  # Unix timestamp (time.time())
}
```

### Conversation Storage (in-memory)

```python
conversations: dict[str, list[dict]]
# Ключ: "chat_id:user_id" (составной ключ)
# Значение: список сообщений (история диалога)
```

### Пример структуры данных

```python
{
    "123456789:987654321": [  # "chat_id:user_id"
        {"role": "system", "content": "Ты помощник...", "timestamp": 1699000000.0},
        {"role": "user", "content": "Привет!", "timestamp": 1699000001.0},
        {"role": "assistant", "content": "Здравствуйте!", "timestamp": 1699000002.5},
        {"role": "user", "content": "Как дела?", "timestamp": 1699000010.0},
        {"role": "assistant", "content": "Отлично!", "timestamp": 1699000012.3}
    ]
}
```

### Особенности

- **Никаких БД** - вся история в RAM
- **Простые структуры** - dict и list
- **Pydantic** только для Config (валидация при запуске)
- **Составной ключ** - "chat_id:user_id" для разделения истории пользователей в групповых чатах
- **Timestamp** - для каждого сообщения (может пригодиться для отладки и логирования)
- **История ограничена** - храним только последние N сообщений для экономии токенов
- **System prompt** один на всех пользователей (задается в .env)
- **Настраиваемые параметры LLM** - temperature, max_tokens, timeout в конфиге
- **При перезапуске история теряется** - это нормально для MVP

## 6. Работа с LLM

### LLMClient - основной класс

**Основной метод:**
```python
async def get_response(self, messages: list[dict]) -> str
```

### Реализация

1. **Использование OpenAI SDK** с базовым URL OpenRouter:
   ```python
   base_url = "https://openrouter.ai/api/v1"
   ```

2. **Формат запроса:**
   - Принимает список сообщений в формате OpenAI (только role и content, без timestamp)
   - Отправляет в OpenRouter с параметрами из конфига
   - Возвращает текст ответа ассистента

3. **Параметры запроса:**
   - `model` - из конфига
   - `messages` - история диалога (role + content)
   - `temperature` - из конфига
   - `max_tokens` - из конфига
   - `timeout` - из конфига

4. **Обработка ошибок:**
   - Timeout - возвращаем "Превышено время ожидания ответа"
   - API Error - логируем и возвращаем "Ошибка при обращении к LLM"
   - Network Error - возвращаем "Проблемы с сетью, попробуйте позже"

### Особенности

- **Простой интерфейс** - один метод для всего
- **Без retry** - пока не усложняем (можно добавить потом)
- **Без streaming** - получаем полный ответ сразу
- **Без кеширования** - каждый запрос новый
- **Асинхронность** - await для API запроса
- **Фильтрация timestamp** - перед отправкой в LLM удаляем timestamp из messages

### Пример использования

```python
llm_client = LLMClient(config)
messages = [
    {"role": "system", "content": "Ты помощник..."},
    {"role": "user", "content": "Привет!"}
]
response = await llm_client.get_response(messages)
```

## 7. Сценарии работы

### Сценарий 1: Первое обращение к боту

1. Пользователь отправляет `/start`
2. Бот отвечает приветственным сообщением с описанием возможностей
3. Создается новая история диалога с system prompt

### Сценарий 2: Обычный диалог

1. Пользователь отправляет текстовое сообщение
2. MessageHandler:
   - Получает chat_id и user_id
   - Формирует ключ "chat_id:user_id"
   - Добавляет сообщение пользователя в историю с timestamp
   - Получает полную историю (включая system prompt)
   - Передает историю (без timestamp) в LLMClient
3. LLMClient отправляет запрос в OpenRouter
4. Получает ответ от LLM
5. MessageHandler:
   - Добавляет ответ ассистента в историю с timestamp
   - Отправляет ответ пользователю в Telegram

### Сценарий 3: Ограничение истории

1. Если история превышает max_history_length:
   - Сохраняем system prompt (первое сообщение)
   - Удаляем самые старые сообщения пользователя и ассистента
   - Оставляем последние N сообщений

### Сценарий 4: Сброс истории диалога

1. Пользователь отправляет `/reset`
2. Бот удаляет всю историю для ключа "chat_id:user_id"
3. Бот отвечает: "История диалога сброшена"
4. При следующем сообщении создается новая история с system prompt

### Сценарий 5: Работа в групповом чате

1. Несколько пользователей пишут в одном чате
2. Каждый пользователь имеет свою историю (благодаря ключу "chat_id:user_id")
3. Истории не пересекаются

### Сценарий 6: Обработка ошибок

1. Если LLM не отвечает - пользователь получает сообщение об ошибке
2. История сохраняется, пользователь может повторить запрос

### Доступные команды

- `/start` - запуск бота и приветствие
- `/reset` - сброс истории диалога

## 8. Подход к конфигурированию

### Конфигурация через .env

**Файл `.env` (не коммитится в git):**
```env
# Telegram Bot
TELEGRAM_BOT_TOKEN=your_bot_token_here

# OpenRouter API
OPENROUTER_API_KEY=your_api_key_here
OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# LLM Parameters
TEMPERATURE=0.7
MAX_TOKENS=1000
TIMEOUT=60

# Bot Settings
SYSTEM_PROMPT=Ты полезный AI-ассистент. Отвечай кратко и по существу.
MAX_HISTORY_LENGTH=10
```

**Файл `.env.example` (коммитится в git):**
```env
# Telegram Bot
TELEGRAM_BOT_TOKEN=

# OpenRouter API
OPENROUTER_API_KEY=
OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# LLM Parameters
TEMPERATURE=0.7
MAX_TOKENS=1000
TIMEOUT=60

# Bot Settings
SYSTEM_PROMPT=Ты полезный AI-ассистент. Отвечай кратко и по существу.
MAX_HISTORY_LENGTH=10
```

### Класс Config

- Использует **Pydantic BaseSettings**
- Автоматически загружает переменные из `.env`
- Валидирует типы данных при запуске
- Падает с понятной ошибкой, если обязательные параметры не заданы

### Особенности

- **Простота** - один файл .env со всеми настройками
- **Безопасность** - секреты не попадают в git (.env в .gitignore)
- **Валидация** - Pydantic проверяет типы и обязательные поля
- **Пример для пользователя** - .env.example показывает, что нужно заполнить
- **Никаких дополнительных файлов конфигурации** - только .env

## 9. Подход к логгированию

### Настройка логгирования

**Конфигурация:**
- Использовать стандартный модуль `logging` из Python
- Вывод в `stdout` (консоль)
- Формат: `%(asctime)s - %(name)s - %(levelname)s - %(message)s`
- Уровень: `INFO` для production

### Что логируем

**INFO уровень:**
- Запуск бота
- Получение сообщения от пользователя (chat_id, user_id, текст обрезанный до 50 символов)
- Отправка ответа пользователю
- Вызов команд (/start, /reset)
- Успешные запросы к LLM (количество токенов)

**ERROR уровень:**
- Ошибки при обращении к LLM API
- Ошибки Telegram API
- Timeout ошибки
- Неожиданные исключения

### Что НЕ логируем

- ❌ Полные тексты сообщений (приватность)
- ❌ API ключи и токены
- ❌ DEBUG информацию (пока не нужна для MVP)
- ❌ Трейсбеки в INFO (только в ERROR)

### Особенности

- **Простота** - только стандартный logging, без дополнительных библиотек
- **Никаких файлов** - только stdout (контейнеры потом подхватят)
- **Минимум информации** - только важное
- **Приватность** - не логируем полные сообщения пользователей
- **Отладка** - достаточно информации для понимания, что происходит

### Пример логов

```
2024-10-10 12:00:00 - bot - INFO - Bot started
2024-10-10 12:00:05 - handlers - INFO - Message from user 123456 in chat 789: "Привет, как дела?..."
2024-10-10 12:00:07 - llm_client - INFO - LLM response received (150 tokens)
2024-10-10 12:00:08 - handlers - INFO - Response sent to user 123456
2024-10-10 12:01:00 - handlers - INFO - Command /reset from user 123456 in chat 789
2024-10-10 12:02:00 - llm_client - ERROR - OpenRouter API timeout after 60s
```

---

## Итого

Данный документ описывает техническое видение для создания простого LLM-ассистента в виде Telegram-бота. Следуя принципам KISS и ООП, мы получаем:

- **6 файлов кода** (main, config, bot, handlers, llm_client, conversation)
- **Минимум зависимостей** (aiogram, openai, pydantic, python-dotenv)
- **Простая архитектура** без избыточных абстракций
- **Готовность к запуску** за минимальное время

Этот проект служит основой для быстрой проверки идеи и может быть легко расширен в будущем при необходимости.


